{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "save_livedoor_corpus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN8V4hfOK7n42Kz1DQswBNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techYamasaki/TopicModelYouTube/blob/master/save_livedoor_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2sBSkp-1mCv",
        "colab_type": "text"
      },
      "source": [
        "# livedoorコーパスの保存"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ9hEswKunQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "!tar xvf ldcc-20140209.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV0hKj7qwuLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sudachipy\n",
        "!pip install pyLDAvis\n",
        "!pip install SudachiPy\n",
        "!pip install https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_core-20190927.tar.gz\n",
        "from gensim import corpora, models\n",
        "from os import listdir, path\n",
        "from sudachipy import tokenizer, dictionary\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RifXPhTwyoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SudachiAnalizer():\n",
        "    \n",
        "    def get_token(self, source) :\n",
        "        \n",
        "        tokenizer_obj = dictionary.Dictionary().create()\n",
        "\n",
        "        mode = tokenizer.Tokenizer.SplitMode.C\n",
        "        result = tokenizer_obj.tokenize(source, mode)\n",
        "\n",
        "        word_list = []\n",
        "        for mrph in result:\n",
        "            if not (mrph == \"\"):\n",
        "                norm_word = mrph.normalized_form()\n",
        "                hinsi = mrph.part_of_speech()[0] \n",
        "\n",
        "                # 単語の正規表現が特定の品詞の場合のみ採用する\n",
        "                if hinsi in  [\"名詞\", \"動詞\", \"形容詞\"]:\n",
        "                    word = norm_word\n",
        "                    word_list.append(word)\n",
        "\n",
        "        return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGBCkS3Ww5oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = []\n",
        "PATH = \"text/\"\n",
        "\n",
        "sudachi = SudachiAnalizer()\n",
        "#  pathの中のdir(txt以外)をlistにして返す\n",
        "def corpus_subdirs(path):\n",
        "    subdirs = []\n",
        "    for x in listdir(path):\n",
        "        if not x.endswith('.txt'):\n",
        "            subdirs.append(x)\n",
        "    return subdirs\n",
        "\n",
        "# pathの中のファイルをlistにして返す\n",
        "def corpus_filenames(path):\n",
        "    labels = [] # *.txt\n",
        "    for y in listdir(path):\n",
        "        if not y.startswith('LICENSE'):\n",
        "            labels.append(y)\n",
        "    return labels\n",
        "\n",
        "for dir in corpus_subdirs(PATH):\n",
        "    for file in corpus_filenames(PATH+dir):\n",
        "        corpus_data = open(path.join(PATH + dir + \"/\" + file), \"r\")\n",
        "        source = corpus_data.read()\n",
        "        token = sudachi.get_token(source)\n",
        "        corpus_data = {\"name\" : file, \"tag\" : dir, \"token\" : token}\n",
        "        docs.append(corpus_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF2BihKpxLwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('corpus.pkl', 'wb') as temp:\n",
        "  pickle.dump(docs, temp)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}